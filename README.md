# Aussie NLP Toolkit

## Overview
The Aussie NLP Toolkit is a modular Python library designed for cleaning, processing, and preparing datasets for training large language models (LLMs), with a focus on Australian-specific contexts. Its modular design encourages contributions, simplifies debugging, and ensures scalability, making it a perfect foundation for collaborative NLP research and projects.

Whether you're processing `.au` domain websites, legal archives, scientific publications, or news datasets, this toolkit provides highly customizable tools tailored for an "Aussie-flavored" dataset preparation pipeline.

---

## Features
- **Modular Design**: Highly focused modules for each task make debugging easier and allow seamless scalability.
- **Beginner-Friendly**: Comprehensive documentation and guides provide an approachable framework for contributors.
- **Error Handling**: Custom exceptions and centralized error-handling mechanisms ensure stability and robustness.
- **Automated Testing and CI/CD**: Integrated testing workflows to maintain code quality across contributions.
- **Australian-Specific Preprocessing**: Tools to handle Aussie slang, regional spelling nuances, `.au` domains, and more.

---

## Pipeline Overview

The Aussie NLP Toolkit processes data through distinct stages, providing flexibility and modularity for different tasks. Each stage is handled by specialized modules, ensuring a scalable and maintainable workflow.

1. **Data Loading**  
   Extracts raw data from input files using loader modules (e.g., HTML, JSON, CSV). Outputs raw data as temporary files in `data/loaded/`.

2. **Cleaning**  
   Refines raw data to ensure consistency and quality. Cleaning modules handle tasks such as deduplication, noise removal, and normalization. Outputs cleaned data into `data/cleaned/`.

3. **Tokenization**  
   Processes cleaned text into smaller units, such as sentences or words, for NLP tasks. Outputs tokenized data into `data/processed/`.

4. **Validation**  
   Ensures input/output data meet expected formats and quality standards before passing to downstream workflows. Outputs validated data into `data/processed/`.

5. **Data Generation**  
   Produces final datasets, reports, or predictions, stored in `data/generated/`.

This modular approach allows users to run the entire pipeline or specific stages independently based on their needs.

---

## Directory Structure
The project is organized into a logical structure for clarity, modularity, and scalability:

```
aussie_nlp_toolkit/
├── data/                            # Main directory for all data-related inputs and outputs
│   ├── raw/                         # Contains unprocessed input data
│   │   ├── .gitkeep                 # Placeholder to keep the raw/ directory
│   ├── cleaned/                     # Contains data after cleaning/preprocessing
│   │   ├── .gitkeep                 # Placeholder to keep the cleaned/ directory
│   ├── processed/                   # Fully processed data ready for use in pipelines
│   │   ├── .gitkeep                 # Placeholder to keep the processed/ directory
│   ├── generated/                   # Outputs generated by scripts or pipelines
│   │   ├── .gitkeep                 # Placeholder to keep the generated/ directory
├── aussie-nlp-tools/                # Root directory for the main library code
│   ├── __init__.py                  # Initializes the main library package
│   ├── data_loader/                 # Handles data loading from different formats
│   │   ├── __init__.py              # Initializes the data_loader package
│   │   ├── detect_type.py           # Detects file types (e.g., HTML, JSON, CSV)
│   │   ├── data_loader_html.py      # Parses HTML files
│   │   ├── data_loader_json.py      # Loads JSON data
│   │   ├── data_loader_csv.py       # Reads CSV files
│   │   ├── data_loader_txt.py       # Processes plain text files
│   │   ├── data_loader_pdf.py       # Handles PDF parsing
│   ├── cleaning/                    # Modules for cleaning and normalisation
│   │   ├── __init__.py              # Initializes the cleaning package
│   │   ├── clean_html_tags.py       # Removes HTML tags
│   │   ├── normalise_unicode.py     # Normalises Unicode characters
│   │   ├── boilerplate_remover.py   # Removes irrelevant content (e.g., ads, footers)
│   │   ├── remove_duplicates.py     # Deduplicates datasets
│   │   ├── aussie_spelling_normaliser.py # Normalises Australian spelling
│   │   ├── language_filter.py       # Filters out non-English content
│   ├── tokeniser/                   # Tokenisation modules
│   │   ├── __init__.py              # Initializes the tokeniser package
│   │   ├── split_sentences.py       # Splits text into sentences
│   │   ├── wordpiece_tokeniser.py   # Tokenises text into subwords (e.g., BPE)
│   │   ├── aussie_slang_tokeniser.py # Detects and tokenises Aussie slang
│   ├── filters/                     # Filters for specific content
│   │   ├── __init__.py              # Initializes the filters package
│   │   ├── filters_metadata.py      # Filters metadata (e.g., timestamps, author info)
│   │   ├── filters_domains.py       # Filters `.au` domain-specific content
│   │   ├── filters_citation_format.py # Handles legal/scientific citations
│   ├── deduplication/               # Modules for deduplication
│   │   ├── __init__.py              # Initializes the deduplication package
│   │   ├── deduplicate_minhash.py   # Removes duplicates using MinHash
│   ├── validation/                  # Modules for input/output validation
│   │   ├── __init__.py              # Initializes the validation package
│   │   ├── validate_input.py        # Validates input formats
│   │   ├── validate_output.py       # Checks output consistency
│   ├── output/                      # Handles output formatting
│   │   ├── __init__.py              # Initializes the output package
│   │   ├── write_csv.py             # Saves data to CSV
│   │   ├── write_json.py            # Saves data to JSON
│   │   ├── write_sqlite.py          # Saves data to SQLite databases
│   ├── utils/                       # General utility functions
│   │   ├── __init__.py              # Initializes the utils package
│   │   ├── logging.py               # Handles pipeline logging
│   │   ├── error_handling.py        # Manages custom exceptions and recovery
│   │   ├── progress_bar.py          # Tracks pipeline progress
├── examples/                        # Example scripts and usage workflows
│   ├── example_pipeline.py          # Demonstrates a typical pipeline
├── guides/                          # Beginner-friendly guides and tutorials
│   ├── guide_add_module.md          # Guide for adding new modules
│   ├── guide_write_tests.md         # Guide for writing tests
│   ├── guide_run_pipeline.md        # Guide for running a data pipeline
├── tests/                           # Tests for automated CI/CD validation
│   ├── data_loader/
│   │   ├── test_detect_type.py         # Tests for detect_type.py
│   │   ├── test_data_loader_html.py    # Tests for data_loader_html.py
│   │   ├── test_data_loader_json.py    # Tests for data_loader_json.py
│   │   ├── test_data_loader_csv.py     # Tests for data_loader_csv.py
│   │   ├── test_data_loader_txt.py     # Tests for data_loader_txt.py
│   │   ├── test_data_loader_pdf.py     # Tests for data_loader_pdf.py
│   ├── cleaning/
│   │   ├── test_clean_html_tags.py     # Tests for clean_html_tags.py
│   │   ├── test_normalise_unicode.py   # Tests for normalise_unicode.py
│   │   ├── test_boilerplate_remover.py # Tests for boilerplate_remover.py
│   │   ├── test_remove_duplicates.py   # Tests for remove_duplicates.py
│   │   ├── test_aussie_spelling_normaliser.py # Tests for aussie_spelling_normaliser.py
│   │   ├── test_language_filter.py     # Tests for language_filter.py
│   ├── tokeniser/
│   │   ├── test_split_sentences.py     # Tests for split_sentences.py
│   │   ├── test_wordpiece_tokeniser.py # Tests for wordpiece_tokeniser.py
│   │   ├── test_aussie_slang_tokeniser.py # Tests for aussie_slang_tokeniser.py
│   ├── filters/
│   │   ├── test_filters_metadata.py    # Tests for filters_metadata.py
│   │   ├── test_filters_domains.py     # Tests for filters_domains.py
│   │   ├── test_filters_citation_format.py # Tests for filters_citation_format.py
│   ├── deduplication/
│   │   ├── test_deduplicate_minhash.py # Tests for deduplicate_minhash.py
│   ├── validation/
│   │   ├── test_validate_input.py      # Tests for validate_input.py
│   │   ├── test_validate_output.py     # Tests for validate_output.py
│   ├── output/
│   │   ├── test_write_csv.py           # Tests for write_csv.py
│   │   ├── test_write_json.py          # Tests for write_json.py
│   │   ├── test_write_sqlite.py        # Tests for write_sqlite.py
│   ├── utils/
│   │   ├── test_logging.py             # Tests for logging.py
│   │   ├── test_error_handling.py      # Tests for error_handling.py
│   │   ├── test_progress_bar.py        # Tests for progress_bar.py
├── README.md                           # Comprehensive project overview
├── CONTRIBUTING.md                     # Contribution guidelines
├── LICENCE                             # Placeholder licence file (Aussie spelling!)
├── setup.py                            # Makes the library installable
├── requirements.txt                    # Dependency list
```

### Data Directory Structure

The toolkit includes standardized directories for managing data at various stages of the pipeline:

- **`data/raw/`**  
  Contains unprocessed input files directly from external sources. Files are dynamically classified by the `detect_filetype.py` script.

- **`data/loaded/`**  
  Stores raw data extracted by loader modules, serving as temporary intermediary files before cleaning. Temporary files follow a naming convention, such as `<original_filename>_loaded.<extension>`.

- **`data/cleaned/`**  
  Contains data refined by cleaning scripts, such as deduplicated or normalized text. These outputs are ready for further processing.

- **`data/processed/`**  
  Contains structured data prepared for NLP workflows, including tokenized or validated files.

- **`data/generated/`**  
  Stores outputs generated by the pipeline or models, such as predictions, datasets, or reports.

Each directory includes a `.gitkeep` file to maintain the structure in the repository.

---

## Getting Started
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/aussie-nlp-toolkit.git
   cd aussie-nlp-toolkit
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Run the example pipeline:
   ```bash
   python examples/example_pipeline.py
   ```

---

## Testing
The toolkit uses **pytest** for automated testing and follows a 1-to-1 test-to-script convention:
- Each script in the toolkit has a corresponding `test_*.py` file in the `tests/` directory.
- To run all tests:
   ```bash
   pytest tests/
   ```

### Continuous Integration (CI)
We employ CI workflows (e.g., GitHub Actions) to automatically run tests whenever new code is pushed or a pull request is opened. This ensures code quality and prevents regressions.

---

## Contributing
We welcome contributions of all levels! Here’s how to get started:
1. Fork the repo and create a new branch for your feature.
2. Write your code and corresponding tests in the `tests/` directory.
3. Submit a pull request with clear descriptions of your changes.

Please follow [PEP 8](https://peps.python.org/pep-0008/) for coding standards.

---

## Future Roadmap
- Add parallel data processing for scalability.
- Introduce model-specific tokenization modules (e.g., SentencePiece).
- Develop more guides for new contributors.

---

## Licence
This project is licensed under the MIT Licence. See the `LICENCE` file for details.

---

## Acknowledgments
This project draws inspiration from modern NLP tools like Hugging Face, NLTK, and spaCy. Special thanks to contributors for helping us build an "Aussie-flavored" NLP library!

---

